Before I get into the nitty gritty of my thinking behind how I tested each function- I am using unity to test all my files and following their layout for how to test stuff.All of the testing files are in unity_tests folder. This means that functions are defined before main which goes against the house style guidelines but thought it would be ok in this instance. The source code for the unity tests is in the folder src. Using unity and following its framework has forced me to think of how to test each bit of my code separately which is really useful.Also just a neat way of comparing doubles is great. Here is the link to the website - https://www.throwtheswitch.org/unity
Although it does throw some warnings for comparing floats/doubles because it does things like num==inf and stuff. Not sure if this is a problem?


Basic Idea for testing parsing section was to start with the commands that do not rely on the other commands eg VAR, OP etc. and then build the commands which use purely those and so on. Additionally I tried to first test each command by itself in its own struct before adding in other commands- so created a new struct to test POLISH, then a new struct for SET and so on. Was important to test the position that the functions move the struct to as well as this will be basis for how we move to next command.

For my overall testing of parser.c I have added in two commands to my makefile - run_parser_err which runs a series of invalid files and feeds the error messages into p_error.txt and run_parser_val which runs a series of valid files. These files are in the report_files folder.

Additionally created each of the ADTS separately tested them by themselves in their own test file and then introduced them into the parsing/interpreting functions. The only functions that relate to the ADTS that are not in the ADT files are the freeing of the hash maps because we need to include specific free functions for the implementation of arrays and for functions.

For the interpreting section of the task testing a bit harder because a lot of it is outputing to SDL. Made sense to me to create a data structure that held all the points that were going to be fed to the SDL function so that these could be tested and checked. Another benefit of this is that we can separate out the SDL functions from our interpreter tests using #defines and check if any of our interpreter functions have memory leaks outside of SDL. This is in addition to using a file I found here- https://github.com/Rickodesea/valgrindSup that suppresses SDL memory leak warnings from valgrind when I run the production code.

Same theory went into building and testing the functions - start from the functions that can work by themselves and can be tested easily and then work our way up to functions that rely on these ones etc. 

For an example the order in which I created the first few main functions in interpreter went -

Rotate -no dependancies
get_num - dependancies from parser section 
Get_rotation - dependancy - get_num
move_forward - dependancies - get_num and rotate


This allows for easier testing as we test each function thoroughly before moving onto the next so if there is a problem we do not have to dive deep into multiple layers of functions- instead abstracting away the complexities of rotating around a point or getting numbers as we go. This sort of permeates into how I set up my test files - basically split up the test files into smaller bits eg testing stacks or testing the functions part of my extension then mix different parts of the project together - not sure if its the most efficient way of testing but makes it clear what bit of the project you are testing at each stage and you can compartmentalise a lot easier. Also means you can focus on one specific part of the code at a time so makes spotting bugs and stuff easier. Only downside is it makes my makefile huge.


For testing how everything runs in regards to the interpreting stage I did a similar thing to the parsing stage where I threw a load of invalid inputs into the makefile and sent the outputs to i_error.txt. Added to this in turtle outputs you can see the output of different files - the name of the picture correlates to the file ran. I did the same thing for the extension as well.

Also not sure if this is the correct place to say this but I put functions that are specific to each part of the project into their own specific.c and specific.h along with struct definitions. 

For black box testing of the interpreter and the parser main bit I asked other students if they wanted to swap files with me so we could see how our code works on unseen files. I have named each of the folders in the test files after the people who gave me the files. As in if Tom Hanks gave me some files the folder is called BB_Tom_Hanks. 


Lastly for the extension I followed pretty much the same ideas as the interpreter - although I split up the extension into three parts, one for functions, one for control flow and one for arrays. This meant that I was able to test each part easily without worrying about their interactions first. 

One thing I didn't test for throughout the files is testing to see how the functions handle a NULL word container - this is because there is no situation where I'm going to be giving the functions NULLs so seemed a bit pointless. Like at the start of every run we create a word container and a line container so I know that the functions will receive non null values. I could have potentially included guards against NULL values but because of the above reasons it just sort of bloats the code a bit for not that much gain.

As a last bit of testing I implemented Rule 110 only using turtle code to make sure everything worked well with each other. Mostly because I couldn't test my extension code on other peoples turtle files so thought the next best thing would be to do a preset problem. 

For testing the debugger I did something a bit extra. I got a working basic version done and then basically just gave out the executable to other people on the course - because I had a bit of time to get ahead a lot of people were still doing the interpreting stage so some of them were quite keen to use it (and help me find bugs). I was reading this thing from Spotify about how Spotify build their products using continuous improvement and shipping regularly and wanted to sort of emulate that. Obviously not going to be that realistic but was fun to have a go. I will include the pdf from Spotify in the zip file its actually a really cool read either way not sure if you'd be interested but I found it interesting and informative. This is in addition to all the usual testing I  did for all the other parts. This has the added benefit of people giving me suggestions for improvements to debugger that I haven't thought about and also does some form of black box testing as well because I have no control over what files they use/what commands they do.


Also when making the debugger I tried to use as many of the functions from previous sections as possible - not sure if this is testing but by putting the functions to use in a different way made it easier to spot limitations and mistakes I had made in my previous code.
